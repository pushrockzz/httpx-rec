name: Receive and Process httpx Chunks

on:
  workflow_dispatch:
    inputs:
      # --- Inputs from the Primary Account's Trigger ---
      primary_github_server_url:
        description: 'The server URL of the primary GitHub instance (e.g., https://github.com)'
        required: true
      primary_repo_owner:
        description: 'The owner of the primary repository that triggered this workflow.'
        required: true
      primary_repo_name:
        description: 'The name of the primary repository.'
        required: true
      primary_run_id:
        description: 'The run_id of the primary workflow to download artifacts from.'
        required: true
      chunk_package_artifact_name:
        description: 'The name of the artifact containing all chunks and resolvers.'
        required: true
      secondary_matrix_json:
        description: 'The JSON string representing the matrix of chunks this account should process.'
        required: true

permissions:
  actions: read      # To download artifacts from the primary repository's workflow run
  contents: write    # To upload its own result artifacts

jobs:
  resolve_secondary_account_chunks:
    name: Resolve Secondary Account Chunks (httpx + dsieve)
    runs-on: ubuntu-latest
    container:
      # Use the exact same container for consistency with the primary account
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ secrets.GHCR_USER }}
        password: ${{ secrets.GHCR_TOKEN }}
    strategy:
      fail-fast: false
      # The matrix is dynamically built from the input payload sent by Account 1
      matrix:
        pair: ${{ fromJson(github.event.inputs.secondary_matrix_json) }}
    steps:
      - name: Checkout repository (for results structure)
        uses: actions/checkout@v3

      - name: Download Full Chunks Package from Primary Account
        env:
          GH_TOKEN_PRIMARY_ACCOUNT_READ: ${{ secrets.PAT_FOR_PRIMARY_ACCOUNT_ARTIFACTS_READ }}
          PRIMARY_REPO_OWNER: ${{ github.event.inputs.primary_repo_owner }}
          PRIMARY_REPO_NAME: ${{ github.event.inputs.primary_repo_name }}
          PRIMARY_RUN_ID: ${{ github.event.inputs.primary_run_id }}
          ARTIFACT_NAME_FROM_PRIMARY: ${{ github.event.inputs.chunk_package_artifact_name }}
        shell: bash
        run: |
          echo "SECONDARY WORKER: Downloading artifact '$ARTIFACT_NAME_FROM_PRIMARY' from $PRIMARY_REPO_OWNER/$PRIMARY_REPO_NAME, run ID $PRIMARY_RUN_ID"
          if ! command -v gh &> /dev/null; then
            echo "INFO: gh CLI not found. Installing..."
            apt-get update -qy
            apt-get install -qy curl
            curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
            chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null
            apt-get update -qy
            apt-get install -qy gh
            if ! command -v gh &> /dev/null; then
              echo "ERROR: gh CLI installation failed."
              exit 1
            fi
          fi

          echo "$GH_TOKEN_PRIMARY_ACCOUNT_READ" | gh auth login --with-token
          FULL_PRIMARY_REPO="$PRIMARY_REPO_OWNER/$PRIMARY_REPO_NAME"
          gh run download "$PRIMARY_RUN_ID" -R "$FULL_PRIMARY_REPO" -n "$ARTIFACT_NAME_FROM_PRIMARY" --dir .

          PACKAGE_FILENAME="$ARTIFACT_NAME_FROM_PRIMARY.tar.gz"
          if [ ! -f "$PACKAGE_FILENAME" ]; then
            echo "ERROR: Failed to download '$PACKAGE_FILENAME'."
            exit 1
          fi
          echo "Downloaded '$PACKAGE_FILENAME'."
          
      - name: Extract Chunks for Secondary
        shell: bash
        run: |
          # The artifact is downloaded as a zip, we need to find and extract the tar.gz inside
          PACKAGE_FILENAME="${{ github.event.inputs.chunk_package_artifact_name }}.tar.gz"
          echo "Unzipping the downloaded artifact archive..."
          #unzip "${{ github.event.inputs.chunk_package_artifact_name }}.zip"

          echo "Extracting the main tarball: $PACKAGE_FILENAME..."
          tar -xzvf "$PACKAGE_FILENAME"
          if [ ! -d "chunks" ]; then
             echo "ERROR: 'chunks/' directory not found after extraction!"
             exit 1
          fi
          echo "Extraction complete. Listing contents of 'chunks/':"
          ls -R chunks/

      - name: Set up Go & install httpx + dsieve
        uses: actions/setup-go@v4
        with:
          go-version: '1.23'
          
      - name: Install httpx and dsieve
        run: |
          if command -v dsieve &> /dev/null; then
            echo "dsieve is already installed"
          else
            echo "Installing dsieve..."
            go install github.com/trickest/dsieve@latest
          fi

      - name: Run httpx + pre-dsieve on subdomains + final filtering
        # This step is an exact copy of the logic from the primary account's workflow
        # It uses the matrix context, which is now populated with the secondary chunks
        shell: bash
        run: |
          DOMAIN=${{ matrix.pair.domain }}
          CHUNK_FILE_PATH=${{ matrix.pair.chunk }}
          echo "Processing domain '$DOMAIN' with chunk '$CHUNK_FILE_PATH'..."

          if [ ! -f "$CHUNK_FILE_PATH" ]; then
            echo "ERROR: Chunk file '$CHUNK_FILE_PATH' not found!"
            exit 1
          fi

          echo "-> Generating parent_domains.txt from raw subdomains..."
          dsieve -if "$CHUNK_FILE_PATH" -f 2 | sort -u > parent_domains.txt

          HTTPX_OUT="httpx_output.txt"
          echo "-> Running httpx (100 threads) against '$CHUNK_FILE_PATH'..."
          httpx \
            -l "$CHUNK_FILE_PATH" \
            -threads 100 \
            -sc -cl -vhost -fhr \
            -retries 2 -timeout 30 \
            -delay 100ms \
            -silent \
            -no-color \
            -o "$HTTPX_OUT"

          if [ ! -s "$HTTPX_OUT" ]; then
            echo "No live URLs found in this chunk. Exiting early."
            # Create empty results dir so artifact upload doesn't fail
            mkdir -p results
            exit 0
          fi
          
          OUTPUT_ROOT="results"
          mkdir -p "$OUTPUT_ROOT"

          while read -r parent; do
            mkdir -p "$OUTPUT_ROOT/$parent"
            awk -v b="$parent" -F'/' '
              {
                host = $3
                sub(/:.*/, "", host)
                if (host == b || host ~ ("\\." b "$")) {
                  print $0
                }
              }
            ' "$HTTPX_OUT" > "$OUTPUT_ROOT/$parent/httpx_result.txt"
          done < parent_domains.txt

          echo "-> Split complete for domain '$DOMAIN'."
          mkdir -p results # Ensure dir exists

      - name: Compute SAFE_CHUNK (no slashes)
        run: |
          SAFE_CHUNK="${{ matrix.pair.chunk }}"
          SAFE_CHUNK="$(echo "$SAFE_CHUNK" | tr '/' '_')"
          echo "SAFE_CHUNK=$SAFE_CHUNK" >> $GITHUB_ENV         
      
      - name: Upload Secondary Account httpx+dsieve Results
        uses: actions/upload-artifact@v4
        with:
          # Name the artifact to clearly identify it as coming from the secondary account
          name: secondary_httpx_dsieve_${{ matrix.pair.domain }}_${{ env.SAFE_CHUNK }}
          path: results/
          retention-days: 1
  merge_results:
    name: Merge All Secondary httpx Results
    runs-on: ubuntu-latest
    # This job needs to run only after all the parallel 'resolve' jobs in THIS workflow are complete.
    needs: resolve_secondary_account_chunks
    if: always() # Ensures we merge results even if some runners failed.
    steps:
      - name: Download all Secondary result artifacts
        uses: actions/download-artifact@v4
        with:
          # CHANGED: The pattern now specifically matches artifacts from the Secondary account.
          pattern: secondary_httpx_dsieve_*
          path: temp-artifacts/

      - name: DEBUG - Show Downloaded Artifact Structure
        if: always()
        run: |
          echo "Listing downloaded artifacts structure in 'temp-artifacts':"
          ls -R temp-artifacts/
          
      - name: Consolidate, Merge, and Deduplicate Secondary Results
        id: consolidate
        shell: bash
        run: |
          # This script is identical to the primary account's script, as the logic is universal.
          FINAL_RESULTS_DIR="final-merged-results"
          mkdir -p "$FINAL_RESULTS_DIR"
          SOURCE_ARTIFACTS_DIR="temp-artifacts"
          if [ ! -d "$SOURCE_ARTIFACTS_DIR" ] || [ -z "$(ls -A $SOURCE_ARTIFACTS_DIR)" ]; then
            echo "No artifacts were downloaded. Nothing to merge."
            echo "artifact_created=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          echo "-> Finding all result files..."
          find "$SOURCE_ARTIFACTS_DIR" -type f -name "httpx_result.txt" | while read -r filepath; do
            parent_domain=$(basename "$(dirname "$filepath")")
            mkdir -p "$FINAL_RESULTS_DIR/$parent_domain"
            cat "$filepath" >> "$FINAL_RESULTS_DIR/$parent_domain/httpx_result.txt"
          done
          echo "-> Aggregation complete. Deduplicating..."
          find "$FINAL_RESULTS_DIR" -type f -name "*.txt" -print -exec sort -u -o {} {} \;
          echo "-> Final results merged and deduplicated."
          ls -R "$FINAL_RESULTS_DIR"
          echo "artifact_created=true" >> $GITHUB_OUTPUT

      - name: Upload Final Merged Secondary Artifact
        if: steps.consolidate.outputs.artifact_created == 'true'
        uses: actions/upload-artifact@v4
        with:
          # CHANGED: The name clearly identifies this artifact as the merged results from the Secondary account.
          name: secondary-httpx-dsieve-merged-results
          path: final-merged-results/
          retention-days: 1

  create_commit:
    name: Create Commit in store-recon (Secondary)
    runs-on: ubuntu-latest
    needs: merge_results
    steps:
      - name: Download Merged Secondary Results Artifact
        uses: actions/download-artifact@v4
        with:
          # CHANGED: Downloads the specific merged artifact from this workflow's 'merge_results' job.
          name: secondary-httpx-dsieve-merged-results
          path: final-merged-results/

      - name: Checkout store-recon Repository
        uses: actions/checkout@v4
        with:
          repository: ${{ secrets.STORE_OWNER }}/${{ secrets.STORE }}
          token: ${{ secrets.PAT_FOR_STORE_REPO }} # Using a generic secret name
          path: store-recon

      - name: Synchronize and Push with Retry
        # This script is identical to the primary one. It is designed to handle race conditions
        # from multiple workflows trying to commit at the same time.
        env:
          SOURCE_DIR: final-merged-results
          STORE: ${{ secrets.STORE }}
          PAT_FOR_STORE_REPO: ${{ secrets.PAT_FOR_STORE_REPO }}
        run: |
          cd store-recon
          git config --global user.name "GitHub Actions Bot (Secondary)"
          git config --global user.email "github-actions-bot@users.noreply.github.com"
          for attempt in {1..5}; do
            echo "[Attempt ${attempt}]"
            git pull --rebase origin main
            MERGE_PERFORMED=false
            for domain_dir in "../$SOURCE_DIR"/*; do
              if [ -d "$domain_dir" ]; then
                domain_name=$(basename "$domain_dir")
                source_file="$domain_dir/httpx_result.txt"
                dest_file="results/$domain_name/httpx_result.txt"
                mkdir -p "$(dirname "$dest_file")" && touch "$dest_file"
                awk 'FNR==NR {data[$1]=$0; next} {data[$1]=$0} END {for (key in data) print data[key]}' "$dest_file" "$source_file" | sort -u > "${dest_file}.tmp"
                mv "${dest_file}.tmp" "$dest_file"
                MERGE_PERFORMED=true
              fi
            done
            if [ "$MERGE_PERFORMED" = false ]; then exit 0; fi
            if git diff --quiet --exit-code; then echo "✅ No new unique data to commit."; exit 0; fi
            git add results/
            git commit -m "feat(recon): Automated results from Secondary Account" -m "Update httpx results from primary workflow run ${{ github.event.inputs.primary_run_id }}."
            if git push -v origin main; then echo "✅ Push successful!"; exit 0; fi
            git reset --hard origin/main
            sleep $((RANDOM % 5 + 2))
          done
          echo "❌ Could not push changes after multiple attempts." && exit 1
